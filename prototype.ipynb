{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b98bb96-89b6-4055-80f8-089c8643bd6c",
   "metadata": {},
   "source": [
    "- Models\n",
    "    - distilbert\n",
    "        - https://huggingface.co/distilbert-base-uncased-distilled-squad\n",
    "        - word count restricted to 512\n",
    "        - appropriate for page summaries\n",
    "    - look into\n",
    "        - longform\n",
    "            - https://medium.com/dair-ai/longformer-what-bert-should-have-been-78f4cd595be9\n",
    "    - model open directory\n",
    "        - https://huggingface.co/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c108b1-b897-455e-af13-604e347bb46c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb1c037d-237f-4db7-8323-2bc8b0afb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for distilbert - answer questions\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# for choosing the correct article to answer question\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# for getting wikipedia articles\n",
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98241bf5-94c3-4a8d-824f-95a2425878b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "47908907-b74d-43f9-b918-c44ae0bd8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile corpus\n",
    "wikipedia_pages = '''\n",
    "Artificial intelligence\n",
    "Natural language processing\n",
    "Deep learning\n",
    "Supervised learning\n",
    "Semi-supervised learning\n",
    "Unsupervised learning\n",
    "Statistical classification\n",
    "Regression analysis\n",
    "Federated learning\n",
    "k-anonymity\n",
    "Data anonymization\n",
    "k-means clustering\n",
    "DBSCAN\n",
    "Dimensionality reduction\n",
    "Silhouette (clustering)\n",
    "Davies–Bouldin index\n",
    "Multidimensional scaling\n",
    "Cluster analysis\n",
    "Principal component analysis\n",
    "Isolation forest\n",
    "Unsupervised learning\n",
    "Hierarchical clustering\n",
    "Local outlier factor\n",
    "Kaiser–Meyer–Olkin test\n",
    "Bartlett's test\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa373184-364c-4676-ae5d-db88241511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title_input': wikipedia_pages.strip().splitlines()})\n",
    "df['title'] = ''\n",
    "df['text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "34066d75-6187-4a32-9b37-0b595a07cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, line in df.iterrows():\n",
    "    page_py = wiki_wiki.page(line['title_input'])\n",
    "    df.iloc[idx]['title'] = page_py.title\n",
    "    df.iloc[idx]['text'] = page_py.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1d715-bce3-489b-8e38-46be3e18910c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Match article to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aa28f191-63fc-493d-93af-a84db83899d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████| 1.18k/1.18k [00:00<00:00, 963kB/s]\n",
      "Downloading: 100%|███████████████████████| 190/190 [00:00<00:00, 148kB/s]\n",
      "Downloading: 100%|██████████████████| 10.6k/10.6k [00:00<00:00, 5.24MB/s]\n",
      "Downloading: 100%|███████████████████████| 612/612 [00:00<00:00, 347kB/s]\n",
      "Downloading: 100%|██████████████████████| 116/116 [00:00<00:00, 91.9kB/s]\n",
      "Downloading: 100%|███████████████████| 39.3k/39.3k [00:00<00:00, 398kB/s]\n",
      "Downloading: 100%|██████████████████| 90.9M/90.9M [00:01<00:00, 50.0MB/s]\n",
      "Downloading: 100%|████████████████████| 53.0/53.0 [00:00<00:00, 31.1kB/s]\n",
      "Downloading: 100%|██████████████████████| 112/112 [00:00<00:00, 65.2kB/s]\n",
      "Downloading: 100%|█████████████████████| 466k/466k [00:00<00:00, 945kB/s]\n",
      "Downloading: 100%|███████████████████████| 350/350 [00:00<00:00, 279kB/s]\n",
      "Downloading: 100%|██████████████████| 13.2k/13.2k [00:00<00:00, 7.43MB/s]\n",
      "Downloading: 100%|█████████████████████| 232k/232k [00:00<00:00, 777kB/s]\n",
      "Downloading: 100%|███████████████████████| 349/349 [00:00<00:00, 296kB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [191]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m embedder \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m corpus_embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:153\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_device\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_length(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus = df[\"title\"]+df[\"text\"]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "corpus_embeddings.shape\n",
    "\n",
    "import torch\n",
    "torch.save(corpus_embeddings, 'corpus_embeddings.pt')\n",
    "corpus_embeddingsLoaded= torch.load('corpus_embeddings.pt')\n",
    "\n",
    "query = 'Statistics'\n",
    "query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddingsLoaded, top_k=top_k)\n",
    "hits =hits [0]\n",
    "\n",
    "for hit in hits:\n",
    "    hit_id = hit ['corpus_id']\n",
    "    article_data = df.iloc[hit_id]\n",
    "    title = article_data ['title']\n",
    "    print (\"-\", title, hit ['score'], hit_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8baece-e9ed-425b-95d3-6bd5022d26a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345058b6-f931-42b0-8fde-549b451c0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "73ebb442-dabc-4882-ae0e-6edb9fe41b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distilbert_ask(question, text):\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca4143-6ab9-44da-b5bc-b6b407e210c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e9c3d804-342b-47ee-bc5d-4f78c01dfdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "wikipedia page title: kmeans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "page title: K-means clustering\n",
      "\n",
      "summary:\n",
      "\n",
      "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\n",
      "The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\n",
      "The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n"
     ]
    }
   ],
   "source": [
    "page_title = input('wikipedia page title:')\n",
    "page_py = wiki_wiki.page(page_title)\n",
    "if not page_py.exists():\n",
    "    print('page does not exist')\n",
    "else:\n",
    "    print('\\npage title:', page_py.title)\n",
    "    print('\\nsummary:\\n')\n",
    "    print(page_py.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8c2212a5-3217-4c8f-8a01-b835d78f8083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Category:Articles with short description': Category:Articles with short description (id: ??, ns: 14),\n",
       " 'Category:CS1 French-language sources (fr)': Category:CS1 French-language sources (fr) (id: ??, ns: 14),\n",
       " 'Category:CS1 errors: missing periodical': Category:CS1 errors: missing periodical (id: ??, ns: 14),\n",
       " 'Category:Cluster analysis algorithms': Category:Cluster analysis algorithms (id: ??, ns: 14),\n",
       " 'Category:Short description is different from Wikidata': Category:Short description is different from Wikidata (id: ??, ns: 14)}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_py.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11542e3a-5e11-4bba-94e9-b0c8aa26878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "question:\n",
      " which metric?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'squared euclidean distances'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input('question:\\n')\n",
    "distilbert_ask(question, page_py.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "369a554d-87a9-425f-8be8-2755aaada47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Affinity propagation': Affinity propagation (id: ??, ns: 0),\n",
       " 'Automatic clustering algorithms': Automatic clustering algorithms (id: ??, ns: 0),\n",
       " 'BFR algorithm': BFR algorithm (id: ??, ns: 0),\n",
       " 'BIRCH': BIRCH (id: ??, ns: 0),\n",
       " 'Canopy clustering algorithm': Canopy clustering algorithm (id: ??, ns: 0),\n",
       " 'Chinese whispers (clustering method)': Chinese whispers (clustering method) (id: ??, ns: 0),\n",
       " 'Cluster-weighted modeling': Cluster-weighted modeling (id: ??, ns: 0),\n",
       " 'Cobweb (clustering)': Cobweb (clustering) (id: ??, ns: 0),\n",
       " 'Complete-linkage clustering': Complete-linkage clustering (id: ??, ns: 0),\n",
       " 'Constrained clustering': Constrained clustering (id: ??, ns: 0),\n",
       " 'CURE algorithm': CURE algorithm (id: ??, ns: 0),\n",
       " 'Data stream clustering': Data stream clustering (id: ??, ns: 0),\n",
       " 'DBSCAN': DBSCAN (id: ??, ns: 0),\n",
       " 'Expectation–maximization algorithm': Expectation–maximization algorithm (id: ??, ns: 0),\n",
       " 'FLAME clustering': FLAME clustering (id: ??, ns: 0),\n",
       " 'Fuzzy clustering': Fuzzy clustering (id: ??, ns: 0),\n",
       " 'Hierarchical clustering': Hierarchical clustering (id: ??, ns: 0),\n",
       " 'Hoshen–Kopelman algorithm': Hoshen–Kopelman algorithm (id: ??, ns: 0),\n",
       " 'Information bottleneck method': Information bottleneck method (id: ??, ns: 0),\n",
       " 'Jenks natural breaks optimization': Jenks natural breaks optimization (id: ??, ns: 0),\n",
       " 'K q-flats': K q-flats (id: ??, ns: 0),\n",
       " 'K-means clustering': K-means clustering (id: ??, ns: 0),\n",
       " 'K-means++': K-means++ (id: ??, ns: 0),\n",
       " 'K-medians clustering': K-medians clustering (id: ??, ns: 0),\n",
       " 'K-medoids': K-medoids (id: ??, ns: 0),\n",
       " 'K-SVD': K-SVD (id: ??, ns: 0),\n",
       " 'Linde–Buzo–Gray algorithm': Linde–Buzo–Gray algorithm (id: ??, ns: 0),\n",
       " 'Low-energy adaptive clustering hierarchy': Low-energy adaptive clustering hierarchy (id: ??, ns: 0),\n",
       " 'Mean shift': Mean shift (id: ??, ns: 0),\n",
       " 'Nearest-neighbor chain algorithm': Nearest-neighbor chain algorithm (id: ??, ns: 0),\n",
       " 'Neighbor joining': Neighbor joining (id: ??, ns: 0),\n",
       " 'OPTICS algorithm': OPTICS algorithm (id: ??, ns: 0),\n",
       " 'Pitman–Yor process': Pitman–Yor process (id: ??, ns: 0),\n",
       " 'Quantum clustering': Quantum clustering (id: ??, ns: 0),\n",
       " 'Self-organizing map': Self-organizing map (id: ??, ns: 0),\n",
       " 'SimRank': SimRank (id: ??, ns: 0),\n",
       " 'Single-linkage clustering': Single-linkage clustering (id: ??, ns: 0),\n",
       " 'Spectral clustering': Spectral clustering (id: ??, ns: 0),\n",
       " 'SUBCLU': SUBCLU (id: ??, ns: 0),\n",
       " 'UPGMA': UPGMA (id: ??, ns: 0),\n",
       " \"Ward's method\": Ward's method (id: ??, ns: 0),\n",
       " 'WPGMA': WPGMA (id: ??, ns: 0)}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search by category\n",
    "c = wiki_wiki.page('Category:Cluster analysis algorithms')\n",
    "c.categorymembers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
