{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b98bb96-89b6-4055-80f8-089c8643bd6c",
   "metadata": {},
   "source": [
    "- Models\n",
    "    - distilbert\n",
    "        - https://huggingface.co/distilbert-base-uncased-distilled-squad\n",
    "        - word count restricted to 512\n",
    "        - appropriate for page summaries\n",
    "    - look into\n",
    "        - longform\n",
    "            - https://medium.com/dair-ai/longformer-what-bert-should-have-been-78f4cd595be9\n",
    "    - model open directory\n",
    "        - https://huggingface.co/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c108b1-b897-455e-af13-604e347bb46c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1c037d-237f-4db7-8323-2bc8b0afb2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for distilbert - answer questions\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# for choosing the correct article to answer question\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# for getting wikipedia articles\n",
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# utils\n",
    "import tqdm\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c51523-f4cd-442a-8793-abed03f9cdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98241bf5-94c3-4a8d-824f-95a2425878b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cc5bd-fc67-443e-956d-58d0b994f7ba",
   "metadata": {},
   "source": [
    "## Curated collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47908907-b74d-43f9-b918-c44ae0bd8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile corpus\n",
    "curated_pages = '''\n",
    "Artificial intelligence\n",
    "Natural language processing\n",
    "Deep learning\n",
    "Supervised learning\n",
    "Semi-supervised learning\n",
    "Unsupervised learning\n",
    "Statistical classification\n",
    "Regression analysis\n",
    "Federated learning\n",
    "k-anonymity\n",
    "Data anonymization\n",
    "k-means clustering\n",
    "DBSCAN\n",
    "Dimensionality reduction\n",
    "Silhouette (clustering)\n",
    "Davies–Bouldin index\n",
    "Multidimensional scaling\n",
    "Cluster analysis\n",
    "Principal component analysis\n",
    "Isolation forest\n",
    "Unsupervised learning\n",
    "Hierarchical clustering\n",
    "Local outlier factor\n",
    "Kaiser–Meyer–Olkin test\n",
    "Bartlett's test\n",
    "\n",
    "Affinity propagation\n",
    "Automatic clustering algorithms\n",
    "BFR algorithm\n",
    "BIRCH\n",
    "Canopy clustering algorithm\n",
    "Chinese whispers (clustering method)\n",
    "Cluster-weighted modeling\n",
    "Cobweb (clustering)\n",
    "Complete-linkage clustering\n",
    "Constrained clustering\n",
    "CURE algorithm\n",
    "Data stream clustering\n",
    "DBSCAN\n",
    "Expectation–maximization algorithm\n",
    "FLAME clustering\n",
    "Fuzzy clustering\n",
    "Hierarchical clustering\n",
    "Hoshen–Kopelman algorithm\n",
    "Information bottleneck method\n",
    "Jenks natural breaks optimization\n",
    "K q-flats\n",
    "K-means clustering\n",
    "K-means++\n",
    "K-medians clustering\n",
    "K-medoids\n",
    "K-SVD\n",
    "Linde–Buzo–Gray algorithm\n",
    "Low-energy adaptive clustering hierarchy\n",
    "Mean shift\n",
    "Nearest-neighbor chain algorithm\n",
    "Neighbor joining\n",
    "OPTICS algorithm\n",
    "Pitman–Yor process\n",
    "Quantum clustering\n",
    "Self-organizing map\n",
    "SimRank\n",
    "Single-linkage clustering\n",
    "Spectral clustering\n",
    "SUBCLU\n",
    "UPGMA\n",
    "Ward's method\n",
    "WPGMA\n",
    "\n",
    "\n",
    "\n",
    "Support-vector machine\n",
    "Boosting (machine learning)\n",
    "Random forest\n",
    "Linear regression\n",
    "Logistic regression\n",
    "Naive Bayes classifier\n",
    "Artificial neural network\n",
    "Perceptron\n",
    "k-nearest neighbors algorithm\n",
    "Semi-supervised learning\n",
    "Ensemble learning\n",
    "Bootstrap aggregating\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "curated_pages = curated_pages.strip().splitlines()  # string to list of strings\n",
    "curated_pages = [p for p in curated_pages if p.strip() != '']  # remove blank lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9666b-1e44-4a3b-b741-a99af5c5d803",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collect all pages under Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8a062-4225-4d5c-8274-bd79da44fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_open = ['Category:Machine learning']\n",
    "cats_close = []\n",
    "all_pages = []\n",
    "while len(cats_open) > 0:\n",
    "    c = cats_open.pop()\n",
    "    if c in cats_close:\n",
    "        continue\n",
    "    cats_close.append(c)\n",
    "    cat = wiki_wiki.page(c)\n",
    "    members = list(cat.categorymembers.keys())\n",
    "    subcats = filter(lambda m: 'Category:' in m, members)\n",
    "    pages = filter(lambda m: 'Category:' not in m, members)\n",
    "    all_pages.extend(pages)\n",
    "    cats_open.extend(subcats)\n",
    "    \n",
    "# remove duplicates\n",
    "auto_pages = []\n",
    "for p in all_pages:\n",
    "    if p not in auto_pages:\n",
    "        auto_pages.append(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192aebd-ba7d-4622-a510-0d561c148ea4",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae16978-67c4-4110-a5a4-bac04d30e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_TO_USE = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9936fa-c666-4823-b806-45780bf8df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_types = {'curated': curated_pages,\n",
    "                'auto': auto_pages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1179-0fcd-4661-8518-d90273a93bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_pages = corpus_types[CORPUS_TO_USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa373184-364c-4676-ae5d-db88241511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title_input': wikipedia_pages})\n",
    "df['title'] = ''\n",
    "df['summary'] = ''\n",
    "df['text'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34066d75-6187-4a32-9b37-0b595a07cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, line in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    page_py = wiki_wiki.page(line['title_input'])\n",
    "    df.at[idx, 'title'] = page_py.title\n",
    "    df.at[idx, 'text'] = page_py.text\n",
    "    df.at[idx, 'summary'] = page_py.summary\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5136fa9-25db-42cd-b218-6d21ff9c0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'corpus_wikipedia_{CORPUS_TO_USE}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1d715-bce3-489b-8e38-46be3e18910c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Match article to input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ba79b-2ac2-4cef-ab60-8469f731fe72",
   "metadata": {},
   "source": [
    "## Train sentence transformer on corpus and store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28f191-63fc-493d-93af-a84db83899d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "corpus = df[\"title\"]+df[\"text\"]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "corpus_embeddings.shape\n",
    "\n",
    "torch.save(corpus_embeddings, f'corpus_embeddings_{CORPUS_TO_USE}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977ff82-d01c-40a3-87b1-1f73ff4d9494",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327cd97f-11f9-48b9-9751-17ec1c4e6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddingsLoaded = torch.load(f'corpus_embeddings_{CORPUS_TO_USE}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cf4a2-c94a-4750-be9d-22757b16e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is the metric used in k means'\n",
    "query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddingsLoaded, top_k=top_k)\n",
    "hits_idx = list(map(lambda x: x['corpus_id'], hits[0]))\n",
    "\n",
    "for hit in hits:\n",
    "    hit_id = hit ['corpus_id']\n",
    "    article_data = df.iloc[hit_id]\n",
    "    title = article_data ['title']\n",
    "    print (\"-\", title, hit ['score'], hit_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc7bdf1-641d-4c94-97b5-97ea242c0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_articles_top_k(query: str, corpus: pd.DataFrame, embedder, model, top_k: int = 10):\n",
    "    corpus_embeddings_loaded = model\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings_loaded, top_k=top_k)\n",
    "    hits_idx = list(map(lambda x: x['corpus_id'], hits[0]))\n",
    "    \n",
    "    return corpus.iloc[hits_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe030e-573e-47b6-b84c-e501972e9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test = 'what is the metric used in k means'\n",
    "get_related_articles_top_k(query=query_test,\n",
    "                           corpus=df,\n",
    "                           model=corpus_embeddingsLoaded,\n",
    "                           embedder=embedder,\n",
    "                           top_k=7\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8baece-e9ed-425b-95d3-6bd5022d26a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ask Questions on specific text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb8113-9982-45b3-a104-f00cc0560564",
   "metadata": {},
   "source": [
    "## Distilbert - max 512 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345058b6-f931-42b0-8fde-549b451c0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ebb442-dabc-4882-ae0e-6edb9fe41b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distilbert_ask(question, text, tokenizer, model):\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    return tokenizer.decode(predict_answer_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca4143-6ab9-44da-b5bc-b6b407e210c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distilbert - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3d804-342b-47ee-bc5d-4f78c01dfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = input('wikipedia page title:')\n",
    "page_py = wiki_wiki.page(page_title)\n",
    "if not page_py.exists():\n",
    "    print('page does not exist')\n",
    "else:\n",
    "    print('\\npage title:', page_py.title)\n",
    "    print('\\nsummary:\\n')\n",
    "    print(page_py.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe8d0a-d092-4b83-a7d6-9b8386f062ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_py.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11542e3a-5e11-4bba-94e9-b0c8aa26878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input('question:\\n')\n",
    "distilbert_ask(question, page_py.summary, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a87fef-b747-44ad-a82b-fa2d3746c754",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Asking any question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c86faf3-ed3b-4709-8750-176a086ce89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c61434-7ce4-4450-89eb-0e0f9a344be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_TO_USE = 'auto'\n",
    "\n",
    "# related articles\n",
    "corpus_df = pd.read_csv(f'corpus_wikipedia_{CORPUS_TO_USE}.csv')\n",
    "corpus_embeddingsLoaded = torch.load(f'corpus_embeddings_{CORPUS_TO_USE}.pt')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# question asking\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "question_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "question_models = {\n",
    "    'distillbert': {'model': question_model, 'max_words': 512},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cf30233-30a7-4b7b-ae6e-a17f68aa84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str,\n",
    "                 related_articles_model,\n",
    "                 related_articles_embedder,\n",
    "                 related_articles_corpus: pd.DataFrame,\n",
    "                 question_models: dict,\n",
    "                 question_tokenizer,\n",
    "                 related_articles_top_k: int = 5,\n",
    "                 ):\n",
    "    # get all relevant articles\n",
    "    related = get_related_articles_top_k(query=question,\n",
    "                                         corpus=related_articles_corpus,\n",
    "                                         embedder=related_articles_embedder,\n",
    "                                         model=related_articles_model,\n",
    "                                         top_k=related_articles_top_k\n",
    "                                        )\n",
    "    results = pd.DataFrame()\n",
    "    # for each relevant article\n",
    "    for idx, page in related.iterrows():        \n",
    "        results.at[idx, 'article title'] = page['title']\n",
    "        results.at[idx, 'article summary'] = page['summary']\n",
    "        \n",
    "        # for each question asking model\n",
    "        for model_name, model_data in question_models.items():\n",
    "            model = model_data['model']\n",
    "            max_words = model_data['max_words']\n",
    "            \n",
    "            # run model on questin with text\n",
    "            # TODO validate text has less than max_words (crop)\n",
    "            \n",
    "            text = page['summary']\n",
    "            assert len(text.split()) < max_words\n",
    "            assert text.strip() != ''  # context must be provided\n",
    "            answer = distilbert_ask(question=question,\n",
    "                                    text=text,\n",
    "                                    tokenizer=question_tokenizer,\n",
    "                                    model=model)\n",
    "            \n",
    "            results.at[idx, model_name] = answer\n",
    "        \n",
    "    \n",
    "    return related, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6b3d0c-e970-4902-a05a-fca7a9d6cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "219\n",
      "236\n",
      "89\n",
      "236\n",
      "86\n",
      "41\n",
      "316\n",
      "60\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "related, results = \\\n",
    "ask_question('what is k-means metric?',\n",
    "             related_articles_model=corpus_embeddingsLoaded,\n",
    "             related_articles_embedder=embedder,\n",
    "             related_articles_corpus=corpus_df,\n",
    "             question_models=question_models,\n",
    "             question_tokenizer=tokenizer,\n",
    "             related_articles_top_k= 10,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721561fb-fb2b-4ac8-a0f3-59c04011224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_ask(question='what is k-means metric?',\n",
    "               text=results.loc[1245]['article summary'],\n",
    "               tokenizer=tokenizer,\n",
    "               model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e6789fa-f9ef-4d29-b183-cc7ff58c50e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article title</th>\n",
       "      <th>distillbert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>K-medians clustering</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>K-medoids</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>K-means clustering</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>Determining the number of clusters in a data set</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>Dunn index</td>\n",
       "      <td>the dunn index ( di ) ( introduced by j. c. du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Principal geodesic analysis</td>\n",
       "      <td>principal geodesic analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>IDistance</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>Data stream clustering</td>\n",
       "      <td>data stream clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>Automatic clustering algorithms</td>\n",
       "      <td>kmeans metric? [SEP] automatic clustering algo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Hans-Peter Kriegel</td>\n",
       "      <td>german computer scientist and professor at the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article title  \\\n",
       "1247                              K-medians clustering   \n",
       "1248                                         K-medoids   \n",
       "1245                                K-means clustering   \n",
       "1203  Determining the number of clusters in a data set   \n",
       "1215                                        Dunn index   \n",
       "864                        Principal geodesic analysis   \n",
       "725                                          IDistance   \n",
       "1236                            Data stream clustering   \n",
       "1211                   Automatic clustering algorithms   \n",
       "573                                 Hans-Peter Kriegel   \n",
       "\n",
       "                                            distillbert  \n",
       "1247                                              [CLS]  \n",
       "1248                                              [CLS]  \n",
       "1245                                              [CLS]  \n",
       "1203                                              [CLS]  \n",
       "1215  the dunn index ( di ) ( introduced by j. c. du...  \n",
       "864                         principal geodesic analysis  \n",
       "725                                                      \n",
       "1236                             data stream clustering  \n",
       "1211  kmeans metric? [SEP] automatic clustering algo...  \n",
       "573   german computer scientist and professor at the...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer('kmeans metric?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c1b8e-2b6e-4d11-9334-7fb82de821fd",
   "metadata": {},
   "source": [
    "## Model served with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "889801c2-dcc4-404a-96a8-8fd1b0d577be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question):\n",
    "    related, results = \\\n",
    "        ask_question(question,\n",
    "                     related_articles_model=corpus_embeddingsLoaded,\n",
    "                     related_articles_embedder=embedder,\n",
    "                     related_articles_corpus=corpus_df,\n",
    "                     question_models=question_models,\n",
    "                     question_tokenizer=tokenizer,\n",
    "                     related_articles_top_k= 10,\n",
    "                    )\n",
    "    #return results.iloc[0]['answer']\n",
    "    return results[['article title', 'distillbert']]\n",
    "\n",
    "gr_interface = gr.Interface(fn=question_answer, inputs=[ \"text\"], outputs=[\"dataframe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7ffd046-954b-4e4c-ad91-c473ae310e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gr_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d8fb65f-6f4f-4f4d-babc-3f4c0032d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()\n",
    "gr_interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4442c313-0639-4775-8b4a-0dab4df2df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fae5b2bb580>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 599, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/interface.py\", line 486, in <lambda>\n",
      "    lambda *args: self.run_prediction(args)[0]\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/interface.py\", line 666, in run_prediction\n",
      "    prediction = self.fn(*processed_input)\n",
      "  File \"/tmp/ipykernel_274536/3441552286.py\", line 3, in question_answer\n",
      "    ask_question(question,\n",
      "  File \"/tmp/ipykernel_274536/2359519234.py\", line 33, in ask_question\n",
      "    answer = distilbert_ask(question=question,\n",
      "  File \"/tmp/ipykernel_274536/1997006036.py\", line 4, in distilbert_ask\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 868, in forward\n",
      "    distilbert_output = self.distilbert(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 566, in forward\n",
      "    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 132, in forward\n",
      "    embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
      "RuntimeError: The size of tensor a (523) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 599, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/blocks.py\", line 514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/interface.py\", line 486, in <lambda>\n",
      "    lambda *args: self.run_prediction(args)[0]\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/gradio/interface.py\", line 666, in run_prediction\n",
      "    prediction = self.fn(*processed_input)\n",
      "  File \"/tmp/ipykernel_274536/3441552286.py\", line 3, in question_answer\n",
      "    ask_question(question,\n",
      "  File \"/tmp/ipykernel_274536/2359519234.py\", line 33, in ask_question\n",
      "    answer = distilbert_ask(question=question,\n",
      "  File \"/tmp/ipykernel_274536/1997006036.py\", line 4, in distilbert_ask\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 868, in forward\n",
      "    distilbert_output = self.distilbert(\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 566, in forward\n",
      "    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ciafa/mnt_point_3/dasilva/deep_learning_iseg/venv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 132, in forward\n",
      "    embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
      "RuntimeError: The size of tensor a (610) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "gr_interface.launch(server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
